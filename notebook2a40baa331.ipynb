{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197a6b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:37:21.824830Z",
     "iopub.status.busy": "2024-12-17T09:37:21.824477Z",
     "iopub.status.idle": "2024-12-17T09:37:21.992237Z",
     "shell.execute_reply": "2024-12-17T09:37:21.991371Z"
    },
    "papermill": {
     "duration": 0.176229,
     "end_time": "2024-12-17T09:37:21.994087",
     "exception": false,
     "start_time": "2024-12-17T09:37:21.817858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057d9eab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-17T09:37:22.002341Z",
     "iopub.status.busy": "2024-12-17T09:37:22.002102Z",
     "iopub.status.idle": "2024-12-17T09:37:23.966022Z",
     "shell.execute_reply": "2024-12-17T09:37:23.965187Z"
    },
    "papermill": {
     "duration": 1.970122,
     "end_time": "2024-12-17T09:37:23.967982",
     "exception": false,
     "start_time": "2024-12-17T09:37:21.997860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ijepa'...\r\n",
      "remote: Enumerating objects: 32, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\r\n",
      "remote: Total 32 (delta 6), reused 3 (delta 3), pack-reused 5 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (32/32), 33.02 KiB | 8.25 MiB/s, done.\r\n",
      "Resolving deltas: 100% (6/6), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/ijepa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e19a37d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:37:23.977400Z",
     "iopub.status.busy": "2024-12-17T09:37:23.976614Z",
     "iopub.status.idle": "2024-12-17T09:44:38.490651Z",
     "shell.execute_reply": "2024-12-17T09:44:38.489699Z"
    },
    "papermill": {
     "duration": 434.520955,
     "end_time": "2024-12-17T09:44:38.492877",
     "exception": false,
     "start_time": "2024-12-17T09:37:23.971922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-17 09:37:24--  https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.14-300e.pth.tar\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.158.20.43, 108.158.20.111, 108.158.20.21, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.158.20.43|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 10358004345 (9.6G) [application/x-tar]\r\n",
      "Saving to: 'IN1K-vit.h.14-300e.pth.tar'\r\n",
      "\r\n",
      "IN1K-vit.h.14-300e. 100%[===================>]   9.65G  26.2MB/s    in 7m 13s  \r\n",
      "\r\n",
      "2024-12-17 09:44:38 (22.8 MB/s) - 'IN1K-vit.h.14-300e.pth.tar' saved [10358004345/10358004345]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/ijepa/IN1K-vit.h.14-300e.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0674b796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:38.679790Z",
     "iopub.status.busy": "2024-12-17T09:44:38.678859Z",
     "iopub.status.idle": "2024-12-17T09:44:43.080649Z",
     "shell.execute_reply": "2024-12-17T09:44:43.079728Z"
    },
    "papermill": {
     "duration": 4.478257,
     "end_time": "2024-12-17T09:44:43.082663",
     "exception": false,
     "start_time": "2024-12-17T09:44:38.604406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "sys.path.append('/kaggle/working/ijepa/')\n",
    "from src.models.vision_transformer import VisionTransformer\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from functools import partial\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from functools import partial\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c0f9ee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:43.229217Z",
     "iopub.status.busy": "2024-12-17T09:44:43.228837Z",
     "iopub.status.idle": "2024-12-17T09:44:44.671798Z",
     "shell.execute_reply": "2024-12-17T09:44:44.670840Z"
    },
    "papermill": {
     "duration": 1.517675,
     "end_time": "2024-12-17T09:44:44.674125",
     "exception": false,
     "start_time": "2024-12-17T09:44:43.156450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "KITTI_ROOT = \"/kaggle/input\"\n",
    "IMAGE_PATH = \"/kaggle/input/data_object_image_2/training/image_2/\"\n",
    "LABEL_PATH = \"/kaggle/input/data_object_label_2/training/label_2/\"\n",
    "CALIB_PATH = \"/kaggle/input/data_object_calib/training/calib/\"\n",
    "CHECKPOINT_PATH = '/kaggle/working/IN1K-vit.h.14-300e.pth.tar'\n",
    "model = VisionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43350c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:44.829225Z",
     "iopub.status.busy": "2024-12-17T09:44:44.828797Z",
     "iopub.status.idle": "2024-12-17T09:44:44.851804Z",
     "shell.execute_reply": "2024-12-17T09:44:44.851080Z"
    },
    "papermill": {
     "duration": 0.098888,
     "end_time": "2024-12-17T09:44:44.853464",
     "exception": false,
     "start_time": "2024-12-17T09:44:44.754576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pretrained_ijepa(checkpoint_path, model):\n",
    "    \"\"\"\n",
    "    Load pretrained IJEPA weights into the model with proper key mapping\n",
    "    \"\"\"\n",
    "    def verify_state_dict(state_dict):\n",
    "        \"\"\"Verify the state dict has expected structure\"\"\"\n",
    "        # Check if we have encoder prefix\n",
    "        has_encoder = any(k.startswith('encoder.') for k in state_dict.keys())\n",
    "        \n",
    "        required_keys = {\n",
    "            'pos_embed', 'patch_embed.proj.weight', 'blocks.0.norm1.weight'\n",
    "        }\n",
    "        if has_encoder:\n",
    "            required_keys = {f'encoder.{k}' for k in required_keys}\n",
    "        \n",
    "        missing_keys = required_keys - set(state_dict.keys())\n",
    "        if missing_keys:\n",
    "            print(f\"Warning: Missing expected keys: {missing_keys}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        # Load checkpoint with weights_only=True for security\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(\n",
    "            checkpoint_path,\n",
    "            map_location='cuda',\n",
    "            weights_only=True\n",
    "        )\n",
    "        \n",
    "        # First, let's examine the checkpoint structure\n",
    "        print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "        \n",
    "        # Handle different checkpoint formats\n",
    "        if 'model' in checkpoint:\n",
    "            state_dict = checkpoint['model']\n",
    "        elif 'encoder' in checkpoint:\n",
    "            state_dict = checkpoint['encoder']\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        print(\"State dict keys sample:\", list(state_dict.keys())[:5])\n",
    "        \n",
    "        # Create mapping for IJEPA keys to model keys\n",
    "        key_mapping = {\n",
    "            'encoder.pos_embed': 'pos_embed',\n",
    "            'encoder.patch_embed': 'patch_embed',\n",
    "            'encoder.blocks': 'blocks',\n",
    "            'encoder.norm': 'norm',\n",
    "            'pos_embed': 'pos_embed',\n",
    "            'patch_embed': 'patch_embed',\n",
    "            'blocks': 'blocks',\n",
    "            'norm': 'norm'\n",
    "        }\n",
    "            \n",
    "        # Remove 'module.' prefix and map keys\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            # Remove module prefix if present\n",
    "            if k.startswith('module.'):\n",
    "                k = k[7:]\n",
    "            \n",
    "            # Map keys from IJEPA format to model format\n",
    "            mapped_key = k\n",
    "            for old_prefix, new_prefix in key_mapping.items():\n",
    "                if k.startswith(old_prefix):\n",
    "                    mapped_key = k.replace(old_prefix, new_prefix)\n",
    "                    break\n",
    "            \n",
    "            # Verify tensor shapes match\n",
    "            if mapped_key in model.state_dict():\n",
    "                if v.shape != model.state_dict()[mapped_key].shape:\n",
    "                    print(f\"Warning: Shape mismatch for {mapped_key}: checkpoint={v.shape}, model={model.state_dict()[mapped_key].shape}\")\n",
    "                    continue\n",
    "                new_state_dict[mapped_key] = v\n",
    "            else:\n",
    "                print(f\"Skipping key {mapped_key} as it's not in model state dict\")\n",
    "            \n",
    "        # Load weights with error handling\n",
    "        try:\n",
    "            msg = model.load_state_dict(new_state_dict, strict=False)\n",
    "            print(f\"Successfully loaded pretrained weights\")\n",
    "            print(f\"Missing keys: {msg.missing_keys}\")\n",
    "            print(f\"Unexpected keys: {msg.unexpected_keys}\")\n",
    "            \n",
    "            # Initialize missing keys\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in msg.missing_keys:\n",
    "                    if 'weight' in name:\n",
    "                        if 'conv' in name or 'proj' in name:\n",
    "                            nn.init.kaiming_normal_(param, mode='fan_out')\n",
    "                        elif 'norm' in name:\n",
    "                            nn.init.constant_(param, 1.0)\n",
    "                        else:\n",
    "                            nn.init.xavier_uniform_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param, 0)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading state dict: {str(e)}\")\n",
    "            print(\"Keys in checkpoint:\", new_state_dict.keys())\n",
    "            print(\"Model state dict keys:\", model.state_dict().keys())\n",
    "            raise\n",
    "            \n",
    "        # Verify model parameters\n",
    "        zero_params = sum(torch.all(p == 0) for p in model.parameters())\n",
    "        if zero_params > 0:\n",
    "            print(f\"Warning: {zero_params} parameters are all zero\")\n",
    "            \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint from {checkpoint_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights\")\n",
    "        return model\n",
    "    \n",
    "def setup_ijepa_config(config_path=None):\n",
    "    \"\"\"\n",
    "    Setup IJEPA configuration for object detection with correct dimensions\n",
    "    \"\"\"\n",
    "    default_config = {\n",
    "        'model': {\n",
    "            'type': 'vit',\n",
    "            'arch': 'huge',\n",
    "            'patch_size': 14,\n",
    "            'width': 1280,  \n",
    "            'depth': 32,\n",
    "            'num_heads': 16,\n",
    "            'mlp_ratio': 4,\n",
    "            'qkv_bias': True,\n",
    "            'drop_path_rate': 0,\n",
    "            'checkpoint_activations': False,\n",
    "            'image_size': 224,\n",
    "            'embed_dim': 1280,  \n",
    "            'num_heads': 16,   \n",
    "            'mlp_dim': 5120 \n",
    "        },\n",
    "        'data': {\n",
    "            'type': 'kitti',\n",
    "            'image_size': 224,\n",
    "            'num_classes': 8  \n",
    "        }\n",
    "    }\n",
    "\n",
    "    return default_config\n",
    "\n",
    "def setup_ijepa_for_detection():\n",
    "    \"\"\"\n",
    "    Setup IJEPA model for object detection with proper initialization\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'model': {\n",
    "            'image_size': (224, 224),\n",
    "            'patch_size': 14,\n",
    "            'num_classes': 8,  \n",
    "            'dim': 1280,       \n",
    "            'depth': 32,       \n",
    "            'heads': 16,       \n",
    "            'mlp_dim': 5120,   \n",
    "            'dropout': 0.0,\n",
    "            'emb_dropout': 0.0,\n",
    "            'embed_dim': 1280, \n",
    "            'num_heads': 16,  \n",
    "            'mlp_ratio': 4,\n",
    "            'qkv_bias': True,\n",
    "            'in_chans': 3    \n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Initialize ViT with matching IJEPA architecture\n",
    "    model = VisionTransformer(\n",
    "        img_size=config['model']['image_size'],\n",
    "        patch_size=config['model']['patch_size'],\n",
    "        in_chans=config['model']['in_chans'],\n",
    "        num_classes=config['model']['num_classes'],\n",
    "        embed_dim=config['model']['embed_dim'],\n",
    "        depth=config['model']['depth'],\n",
    "        num_heads=config['model']['heads'],\n",
    "        mlp_ratio=config['model']['mlp_ratio'],\n",
    "        qkv_bias=config['model']['qkv_bias'],\n",
    "        drop_rate=config['model']['dropout'],\n",
    "        attn_drop_rate=config['model']['dropout'],\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        init_values=None,\n",
    "        use_abs_pos_emb=True,\n",
    "        init_std=0.02\n",
    "    )\n",
    "\n",
    "    return model, config\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    pretrained_path = CHECKPOINT_PATH\n",
    "    \n",
    "    # Verify checkpoint exists\n",
    "    if not os.path.exists(pretrained_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {pretrained_path}\")\n",
    "        \n",
    "    # Examine checkpoint structure\n",
    "    print(\"Examining checkpoint structure...\")\n",
    "    checkpoint = torch.load(pretrained_path, map_location='cpu', weights_only=True)\n",
    "    print(\"Checkpoint keys:\", checkpoint.keys())\n",
    "    \n",
    "    # Setup model and config with error handling\n",
    "    try:\n",
    "        model, config = setup_ijepa_for_detection()\n",
    "        print(\"Model state dict keys:\", list(model.state_dict().keys())[:5])\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up model: {str(e)}\")\n",
    "        return None, None\n",
    "        \n",
    "    # Load pretrained weights with validation\n",
    "    try:\n",
    "        model = load_pretrained_ijepa(pretrained_path, model)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pretrained weights: {str(e)}\")\n",
    "        \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Print model statistics\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "392551ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:45.001136Z",
     "iopub.status.busy": "2024-12-17T09:44:45.000803Z",
     "iopub.status.idle": "2024-12-17T09:44:45.141384Z",
     "shell.execute_reply": "2024-12-17T09:44:45.140712Z"
    },
    "papermill": {
     "duration": 0.216919,
     "end_time": "2024-12-17T09:44:45.143188",
     "exception": false,
     "start_time": "2024-12-17T09:44:44.926269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DETRDetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, hidden_dim=256, num_queries=100):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        # Transformer components with batch_first=True\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            activation='relu',\n",
    "            batch_first=True  # Set batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=6\n",
    "        )\n",
    "        \n",
    "        # Learnable object queries\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.LayerNorm(in_channels),\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Output FFNs\n",
    "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)  # +1 for no-object\n",
    "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        # Project input features\n",
    "        x = self.input_proj(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Add positional queries\n",
    "        queries = self.query_embed.weight.unsqueeze(0).repeat(bs, 1, 1)  # [batch_size, num_queries, hidden_dim]\n",
    "        x = torch.cat([queries, x], dim=1)  # [batch_size, num_queries + seq_len, hidden_dim]\n",
    "        \n",
    "        # Transformer processing (now with batch_first=True)\n",
    "        memory = self.transformer(x)  # [batch_size, num_queries + seq_len, hidden_dim]\n",
    "        \n",
    "        # Extract query outputs\n",
    "        outputs_class = self.class_embed(memory[:, :self.num_queries])\n",
    "        outputs_coord = self.bbox_embed(memory[:, :self.num_queries]).sigmoid()\n",
    "        \n",
    "        return outputs_class, outputs_coord\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "class IJEPADETRDetector(nn.Module):\n",
    "    def __init__(self, backbone, num_classes, hidden_dim=256, num_queries=100):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.detection_head = DETRDetectionHead(\n",
    "            in_channels=backbone.embed_dim,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_queries=num_queries\n",
    "        )\n",
    "        \n",
    "        # Freeze backbone initially\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.detection_head(features)\n",
    "\n",
    "def create_detection_transforms(image_size=224):\n",
    "    \"\"\"Create transforms for detection\"\"\"\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "class DETRLoss(nn.Module):\n",
    "    def __init__(self, num_classes, matcher_cost_class=1, matcher_cost_bbox=5, matcher_cost_giou=2):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher_cost_class = matcher_cost_class\n",
    "        self.matcher_cost_bbox = matcher_cost_bbox\n",
    "        self.matcher_cost_giou = matcher_cost_giou\n",
    "        \n",
    "        # Register buffers for class weights\n",
    "        self.register_buffer('empty_weight', torch.ones(num_classes + 1))\n",
    "        self.empty_weight[-1] = 0.1  # Lower weight for background class\n",
    "        \n",
    "    def calculate_giou(self, box1, box2):\n",
    "        \"\"\"Calculate GIoU between two boxes in (cx, cy, w, h) format\"\"\"\n",
    "        # Convert to x1,y1,x2,y2\n",
    "        x1_1, y1_1 = box1[..., 0] - box1[..., 2]/2, box1[..., 1] - box1[..., 3]/2\n",
    "        x2_1, y2_1 = box1[..., 0] + box1[..., 2]/2, box1[..., 1] + box1[..., 3]/2\n",
    "        x1_2, y1_2 = box2[..., 0] - box2[..., 2]/2, box2[..., 1] - box2[..., 3]/2\n",
    "        x2_2, y2_2 = box2[..., 0] + box2[..., 2]/2, box2[..., 1] + box2[..., 3]/2\n",
    "        \n",
    "        # Intersection\n",
    "        x1_i = torch.max(x1_1, x1_2)\n",
    "        y1_i = torch.max(y1_1, y1_2)\n",
    "        x2_i = torch.min(x2_1, x2_2)\n",
    "        y2_i = torch.min(y2_1, y2_2)\n",
    "        \n",
    "        intersection = torch.clamp(x2_i - x1_i, min=0) * torch.clamp(y2_i - y1_i, min=0)\n",
    "        \n",
    "        # Union\n",
    "        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        # Enclosing box\n",
    "        x1_e = torch.min(x1_1, x1_2)\n",
    "        y1_e = torch.min(y1_1, y1_2)\n",
    "        x2_e = torch.max(x2_1, x2_2)\n",
    "        y2_e = torch.max(y2_1, y2_2)\n",
    "        \n",
    "        enclosing_area = (x2_e - x1_e) * (y2_e - y1_e)\n",
    "        \n",
    "        # GIoU\n",
    "        iou = intersection / (union + 1e-6)\n",
    "        giou = iou - (enclosing_area - union) / (enclosing_area + 1e-6)\n",
    "        \n",
    "        return giou\n",
    "\n",
    "    def hungarian_matcher(self, outputs, targets):\n",
    "        \"\"\"Match predictions to targets using Hungarian algorithm\"\"\"\n",
    "        bs, num_queries = outputs['pred_logits'].shape[:2]\n",
    "        \n",
    "        # Create cost matrix for each batch\n",
    "        indices = []\n",
    "        for b in range(bs):\n",
    "            out_prob = outputs['pred_logits'][b].softmax(-1)\n",
    "            out_bbox = outputs['pred_boxes'][b]\n",
    "            \n",
    "            # Handle empty targets\n",
    "            if len(targets['labels'][b]) == 0:\n",
    "                indices.append(([], []))\n",
    "                continue\n",
    "                \n",
    "            tgt_ids = targets['labels'][b]\n",
    "            tgt_bbox = targets['boxes'][b]\n",
    "            \n",
    "            # Classification cost\n",
    "            cost_class = -out_prob[:, tgt_ids]\n",
    "            \n",
    "            # L1 cost\n",
    "            cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "            \n",
    "            # GIoU cost\n",
    "            cost_giou = -self.calculate_giou(out_bbox.unsqueeze(1), tgt_bbox.unsqueeze(0))\n",
    "            \n",
    "            # Final cost matrix\n",
    "            C = (self.matcher_cost_class * cost_class + \n",
    "                 self.matcher_cost_bbox * cost_bbox +\n",
    "                 self.matcher_cost_giou * cost_giou)\n",
    "            \n",
    "            # Hungarian matching\n",
    "            indices_b = linear_sum_assignment(C.cpu().detach().numpy())\n",
    "            indices.append(indices_b)\n",
    "            \n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), \n",
    "                 torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Restructure outputs and targets\n",
    "        outputs = {\n",
    "            'pred_logits': outputs[0],  # Classification predictions\n",
    "            'pred_boxes': outputs[1]    # Box predictions\n",
    "        }\n",
    "        \n",
    "        target_dict = {\n",
    "            'labels': [t for t in targets[1]],  # labels_list\n",
    "            'boxes': [b for b in targets[0]]    # boxes_list\n",
    "        }\n",
    "        \n",
    "        # Match predictions to targets\n",
    "        indices = self.hungarian_matcher(outputs, target_dict)\n",
    "        \n",
    "        # Compute losses\n",
    "        num_boxes = sum(len(t) for t in target_dict['labels'])\n",
    "        num_boxes = torch.as_tensor(num_boxes, dtype=torch.float, device=outputs['pred_logits'].device)\n",
    "        \n",
    "        # Classification loss\n",
    "        src_logits = outputs['pred_logits']\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                  dtype=torch.int64, device=src_logits.device)\n",
    "        \n",
    "        for batch_idx, (pred_idx, tgt_idx) in enumerate(indices):\n",
    "            if len(tgt_idx) > 0:  # Only update if we have targets\n",
    "                target_classes[batch_idx, pred_idx] = target_dict['labels'][batch_idx][tgt_idx]\n",
    "        \n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        \n",
    "        # Box losses\n",
    "        loss_bbox = torch.tensor(0., device=src_logits.device)\n",
    "        loss_giou = torch.tensor(0., device=src_logits.device)\n",
    "        \n",
    "        for batch_idx, (pred_idx, tgt_idx) in enumerate(indices):\n",
    "            if len(pred_idx) == 0:\n",
    "                continue\n",
    "                \n",
    "            pred_boxes = outputs['pred_boxes'][batch_idx, pred_idx]\n",
    "            target_boxes = target_dict['boxes'][batch_idx][tgt_idx]\n",
    "            \n",
    "            # L1 loss\n",
    "            loss_bbox += F.l1_loss(pred_boxes, target_boxes, reduction='none').sum() / num_boxes\n",
    "            \n",
    "            # GIoU loss\n",
    "            loss_giou += (1 - self.calculate_giou(pred_boxes, target_boxes)).sum() / num_boxes\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = loss_ce + 5 * loss_bbox + 2 * loss_giou\n",
    "        \n",
    "        loss_dict = {\n",
    "            'loss': loss.item(),\n",
    "            'loss_ce': loss_ce.item(),\n",
    "            'loss_bbox': loss_bbox.item(),\n",
    "            'loss_giou': loss_giou.item()\n",
    "        }\n",
    "        \n",
    "        return loss, loss_dict\n",
    "\n",
    "def train_step(model, batch, criterion, optimizer, device):\n",
    "    \"\"\"Modified training step\"\"\"\n",
    "    images, boxes_list, labels_list = batch\n",
    "\n",
    "    # Move images to device\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Move boxes and labels to device\n",
    "    boxes_list = [boxes.to(device) for boxes in boxes_list]\n",
    "    labels_list = [labels.to(device) for labels in labels_list]\n",
    "\n",
    "    # Forward pass\n",
    "    cls_preds, bbox_preds = model(images)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss, loss_dict = criterion(cls_preds, bbox_preds, labels_list, boxes_list)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebf7f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:45.292577Z",
     "iopub.status.busy": "2024-12-17T09:44:45.292239Z",
     "iopub.status.idle": "2024-12-17T09:44:45.858253Z",
     "shell.execute_reply": "2024-12-17T09:44:45.857585Z"
    },
    "papermill": {
     "duration": 0.64393,
     "end_time": "2024-12-17T09:44:45.860166",
     "exception": false,
     "start_time": "2024-12-17T09:44:45.216236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class KITTIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    KITTI Dataset for Object Detection (Kaggle version)\n",
    "    Directory structure:\n",
    "    kitti_root/\n",
    "        - data_object_image_2/\n",
    "            - training/\n",
    "            - testing/\n",
    "        - data_object_label_2/\n",
    "            - training/\n",
    "        - data_object_calib/\n",
    "            - training/\n",
    "            - testing/\n",
    "    \"\"\"\n",
    "\n",
    "    CLASSES = {\n",
    "        'Car': 0,\n",
    "        'Van': 1,\n",
    "        'Truck': 2,\n",
    "        'Pedestrian': 3,\n",
    "        'Person_sitting': 4,\n",
    "        'Cyclist': 5,\n",
    "        'Tram': 6,\n",
    "        'Misc': 7\n",
    "    }\n",
    "\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to KITTI dataset root directory\n",
    "            split (str): 'train' or 'test'\n",
    "            transform: Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        # Set up paths based on new directory structure\n",
    "        split_folder = 'training' if split == 'train' else 'testing'\n",
    "        self.image_dir = self.root_dir / 'data_object_image_2' / split_folder / 'image_2'\n",
    "        self.label_dir = self.root_dir / 'data_object_label_2' / 'training' / 'label_2' if split == 'train' else None\n",
    "\n",
    "        # Get all image files\n",
    "        self.image_files = sorted(self.image_dir.glob('*.png'))\n",
    "\n",
    "        # Validate dataset structure\n",
    "        self._validate_dataset()\n",
    "\n",
    "    def _validate_dataset(self):\n",
    "        \"\"\"Validate the dataset structure\"\"\"\n",
    "        assert self.image_dir.exists(), f\"Image directory not found at {self.image_dir}\"\n",
    "        if self.split == 'train':\n",
    "            assert self.label_dir.exists(), f\"Label directory not found at {self.label_dir}\"\n",
    "            # Verify matching number of labels and images\n",
    "            label_files = list(self.label_dir.glob('*.txt'))\n",
    "            assert len(label_files) == len(self.image_files), \\\n",
    "                \"Mismatch between number of images and labels\"\n",
    "\n",
    "    def _read_label_file(self, label_file):\n",
    "        \"\"\"\n",
    "        Read KITTI label file and extract object information\n",
    "        Returns:\n",
    "            boxes: numpy array of bounding boxes [N, 4] (x1, y1, x2, y2)\n",
    "            labels: numpy array of class labels [N]\n",
    "        \"\"\"\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ')\n",
    "\n",
    "                # Skip objects we don't want to detect\n",
    "                class_name = parts[0]\n",
    "                if class_name not in self.CLASSES:\n",
    "                    continue\n",
    "\n",
    "                # Extract 2D bounding box (x1, y1, x2, y2)\n",
    "                bbox = [float(parts[4]), float(parts[5]),\n",
    "                       float(parts[6]), float(parts[7])]\n",
    "\n",
    "                boxes.append(bbox)\n",
    "                labels.append(self.CLASSES[class_name])\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        boxes = np.array(boxes, dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def _preprocess_box(self, box, image_size):\n",
    "        \"\"\"\n",
    "        Convert absolute coordinates to normalized coordinates\n",
    "        Args:\n",
    "            box: numpy array [4] (x1, y1, x2, y2)\n",
    "            image_size: tuple (width, height)\n",
    "        Returns:\n",
    "            normalized box coordinates\n",
    "        \"\"\"\n",
    "        width, height = image_size\n",
    "\n",
    "        # Clip coordinates to image boundaries\n",
    "        x1 = np.clip(box[0], 0, width)\n",
    "        y1 = np.clip(box[1], 0, height)\n",
    "        x2 = np.clip(box[2], 0, width)\n",
    "        y2 = np.clip(box[3], 0, height)\n",
    "\n",
    "        # Normalize coordinates\n",
    "        x1 = x1 / width\n",
    "        y1 = y1 / height\n",
    "        x2 = x2 / width\n",
    "        y2 = y2 / height\n",
    "\n",
    "        # Convert to center, width, height format\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        box_width = x2 - x1\n",
    "        box_height = y2 - y1\n",
    "\n",
    "        return np.array([center_x, center_y, box_width, box_height],\n",
    "                       dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of images in the dataset\"\"\"\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        Returns:\n",
    "            image: transformed image tensor\n",
    "            boxes: tensor of bounding boxes [N, 4] (center_x, center_y, width, height)\n",
    "            labels: tensor of class labels [N]\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image_path = self.image_files[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        original_size = image.size  # (width, height)\n",
    "\n",
    "        # Load labels if in training mode\n",
    "        if self.split == 'train':\n",
    "            label_path = self.label_dir / (image_path.stem + '.txt')\n",
    "            boxes, labels = self._read_label_file(label_path)\n",
    "\n",
    "            # Convert boxes to normalized center format\n",
    "            processed_boxes = np.array([\n",
    "                self._preprocess_box(box, original_size) for box in boxes\n",
    "            ])\n",
    "        else:\n",
    "            # For test set, return dummy targets\n",
    "            processed_boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "            labels = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes_tensor = torch.from_numpy(processed_boxes)\n",
    "        labels_tensor = torch.from_numpy(labels)\n",
    "\n",
    "        return image, boxes_tensor, labels_tensor\n",
    "\n",
    "    def get_image_path(self, idx):\n",
    "        \"\"\"Get the path of an image given its index\"\"\"\n",
    "        return str(self.image_files[idx])\n",
    "\n",
    "def create_kitti_dataloader(\n",
    "    root_dir,\n",
    "    split='train',\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    transform=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the KITTI dataset\n",
    "    Args:\n",
    "        root_dir (str): Path to KITTI dataset root\n",
    "        split (str): 'train' or 'test'\n",
    "        batch_size (int): Batch size\n",
    "        num_workers (int): Number of worker processes\n",
    "        transform: Optional transform to be applied on images\n",
    "    Returns:\n",
    "        DataLoader object\n",
    "    \"\"\"\n",
    "    dataset = KITTIDataset(\n",
    "        root_dir=root_dir,\n",
    "        split=split,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(split == 'train'),\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of objects per image\n",
    "    Args:\n",
    "        batch: list of tuples (image, boxes, labels)\n",
    "    Returns:\n",
    "        images: tensor of shape [batch_size, C, H, W]\n",
    "        boxes: list of tensors of shape [N, 4]\n",
    "        labels: list of tensors of shape [N]\n",
    "    \"\"\"\n",
    "    images, boxes, labels = zip(*batch)\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Return as tuple\n",
    "    return images, list(boxes), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab479b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:46.008802Z",
     "iopub.status.busy": "2024-12-17T09:44:46.008338Z",
     "iopub.status.idle": "2024-12-17T09:44:47.447990Z",
     "shell.execute_reply": "2024-12-17T09:44:47.447292Z"
    },
    "papermill": {
     "duration": 1.51607,
     "end_time": "2024-12-17T09:44:47.450020",
     "exception": false,
     "start_time": "2024-12-17T09:44:45.933950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_train_val_split(dataset, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/validation split from the training dataset\n",
    "    \"\"\"\n",
    "    all_indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        all_indices, \n",
    "        test_size=val_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "\n",
    "def create_train_val_dataloaders(root_dir, batch_size=8, val_ratio=0.2, num_workers=2, transform=None):\n",
    "    \"\"\"Create separate train and validation dataloaders from training data\"\"\"\n",
    "    # Create full training dataset\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    full_dataset = KITTIDataset(\n",
    "        root_dir=root_dir,\n",
    "        split='train',  # Use training data for both train and val\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create train/val splits\n",
    "    train_indices, val_indices = create_train_val_split(full_dataset)\n",
    "\n",
    "    # Create subset samplers\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "# Add this validation function before the training function\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Run validation loop\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, boxes_list, labels_list in val_loader:\n",
    "            images = images.to(device)\n",
    "            boxes_list = [boxes.to(device) for boxes in boxes_list]\n",
    "            labels_list = [labels.to(device) for labels in labels_list]\n",
    "            \n",
    "            # Update autocast usage\n",
    "            with autocast(device_type='cuda' if device=='cuda' else 'cpu'):\n",
    "                cls_preds, bbox_preds = model(images)\n",
    "                loss, _ = criterion(cls_preds, bbox_preds, labels_list, boxes_list)\n",
    "            \n",
    "            val_loss.update(loss.item(), images.size(0))\n",
    "            \n",
    "    return val_loss.avg\n",
    "\n",
    "# Replace your existing train_ijepa_detector function with this improved version\n",
    "def train_ijepa_detector(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=30,\n",
    "    initial_lr=1e-4,\n",
    "    device='cuda',\n",
    "    unfreeze_backbone_epoch=5,\n",
    "    max_objects=50,\n",
    "    patience=7,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulation_steps=4\n",
    "):\n",
    "    model = model.to(device)\n",
    "    criterion = DETRLoss(num_classes=8).to(device)  # 8 classes for KITTI\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=initial_lr,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    # Cosine learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for i, (images, boxes_list, labels_list) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            boxes_list = [boxes.to(device) for boxes in boxes_list]\n",
    "            labels_list = [labels.to(device) for labels in labels_list]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)  # Returns (cls_preds, bbox_preds)\n",
    "            \n",
    "            # Format targets for DETR loss\n",
    "            targets = (boxes_list, labels_list)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, loss_dict = criterion(outputs, targets)\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient accumulation\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_val)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(i+1):.4f}',\n",
    "                'cls_loss': f'{loss_dict[\"loss_ce\"]:.4f}',\n",
    "                'bbox_loss': f'{loss_dict[\"loss_bbox\"]:.4f}',\n",
    "                'giou_loss': f'{loss_dict[\"loss_giou\"]:.4f}',\n",
    "                'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "\n",
    "            # Clear cache periodically\n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, boxes_list, labels_list in val_loader:\n",
    "                images = images.to(device)\n",
    "                boxes_list = [boxes.to(device) for boxes in boxes_list]\n",
    "                labels_list = [labels.to(device) for labels in labels_list]\n",
    "                \n",
    "                outputs = model(images)\n",
    "                targets = (boxes_list, labels_list)\n",
    "                loss, _ = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        \n",
    "        val_loss = val_loss / val_steps\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1} - Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Update your SimpleDetectionEvaluator class with these improvements\n",
    "class SimpleDetectionEvaluator:\n",
    "    def __init__(self, model, device, confidence_threshold=0.3):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.classes = {\n",
    "            0: 'Car', 1: 'Van', 2: 'Truck', 3: 'Pedestrian',\n",
    "            4: 'Person_sitting', 5: 'Cyclist', 6: 'Tram', 7: 'Misc'\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_image(self, image):\n",
    "        self.model.eval()\n",
    "        image = image.to(self.device)\n",
    "        if image.dim() == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "            \n",
    "        # Get predictions\n",
    "        cls_preds, bbox_preds = self.model(image)\n",
    "        \n",
    "        # Process predictions\n",
    "        scores = F.softmax(cls_preds[0], dim=-1)\n",
    "        confidences, labels = scores.max(dim=-1)\n",
    "        boxes = bbox_preds[0]\n",
    "        \n",
    "        # Filter by confidence\n",
    "        mask = confidences > self.confidence_threshold\n",
    "        filtered_boxes = boxes[mask]\n",
    "        filtered_scores = confidences[mask]\n",
    "        filtered_labels = labels[mask]\n",
    "        \n",
    "        return filtered_boxes.cpu(), filtered_scores.cpu(), filtered_labels.cpu()\n",
    "\n",
    "# Add this new evaluation function\n",
    "def evaluate_model(model, val_loader, device, num_samples=5):\n",
    "    evaluator = SimpleDetectionEvaluator(model, device)\n",
    "    model.eval()\n",
    "    \n",
    "    metrics = {\n",
    "        'total_detections': 0,\n",
    "        'class_detections': {i: 0 for i in range(8)},\n",
    "        'avg_confidence': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, target_boxes, target_labels) in enumerate(val_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            image = images[0]\n",
    "            boxes, scores, labels = evaluator.predict_image(image)\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics['total_detections'] += len(boxes)\n",
    "            metrics['avg_confidence'].extend(scores.tolist())\n",
    "            for label in labels:\n",
    "                metrics['class_detections'][label.item()] += 1\n",
    "    \n",
    "    # Calculate average confidence\n",
    "    metrics['avg_confidence'] = np.mean(metrics['avg_confidence']) if metrics['avg_confidence'] else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Total detections: {metrics['total_detections']}\")\n",
    "    print(f\"Average confidence: {metrics['avg_confidence']:.4f}\")\n",
    "    print(\"\\nDetections per class:\")\n",
    "    for class_id, count in metrics['class_detections'].items():\n",
    "        print(f\"{evaluator.classes[class_id]}: {count}\")\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "# Update these values in create_train_val_dataloaders\n",
    "batch_size = 8  # Reduced batch size\n",
    "num_workers = 2  # Reduced workers\n",
    "\n",
    "# Update the training transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataloaders with proper batch size\n",
    "train_loader, val_loader = create_train_val_dataloaders(\n",
    "    root_dir=KITTI_ROOT,\n",
    "    batch_size=8,  # Smaller batch size to avoid memory issues\n",
    "    val_ratio=0.2,\n",
    "    num_workers=2,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a2e6e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:47.597644Z",
     "iopub.status.busy": "2024-12-17T09:44:47.597169Z",
     "iopub.status.idle": "2024-12-17T09:44:47.601113Z",
     "shell.execute_reply": "2024-12-17T09:44:47.600408Z"
    },
    "papermill": {
     "duration": 0.078995,
     "end_time": "2024-12-17T09:44:47.602591",
     "exception": false,
     "start_time": "2024-12-17T09:44:47.523596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_OBJECTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b0e3fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:44:47.826548Z",
     "iopub.status.busy": "2024-12-17T09:44:47.826048Z",
     "iopub.status.idle": "2024-12-17T09:45:18.555274Z",
     "shell.execute_reply": "2024-12-17T09:45:18.554570Z"
    },
    "papermill": {
     "duration": 30.848073,
     "end_time": "2024-12-17T09:45:18.557263",
     "exception": false,
     "start_time": "2024-12-17T09:44:47.709190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /kaggle/working/IN1K-vit.h.14-300e.pth.tar\n",
      "Checkpoint keys: dict_keys(['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr'])\n",
      "State dict keys sample: ['module.pos_embed', 'module.patch_embed.proj.weight', 'module.patch_embed.proj.bias', 'module.blocks.0.norm1.weight', 'module.blocks.0.norm1.bias']\n",
      "Successfully loaded pretrained weights\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "# Set up model with correct parameters\n",
    "backbone, config = setup_ijepa_for_detection()\n",
    "backbone = load_pretrained_ijepa(CHECKPOINT_PATH, backbone)\n",
    "\n",
    "# Create detector\n",
    "model = IJEPADETRDetector(\n",
    "    backbone=backbone,\n",
    "    num_classes=len(KITTIDataset.CLASSES),\n",
    "    num_queries=MAX_OBJECTS,\n",
    "    hidden_dim=256\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "240c37b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T09:45:18.705953Z",
     "iopub.status.busy": "2024-12-17T09:45:18.705648Z",
     "iopub.status.idle": "2024-12-17T14:57:23.745092Z",
     "shell.execute_reply": "2024-12-17T14:57:23.744012Z"
    },
    "papermill": {
     "duration": 18728.10068,
     "end_time": "2024-12-17T14:57:26.732322",
     "exception": false,
     "start_time": "2024-12-17T09:45:18.631642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|| 748/748 [06:23<00:00,  1.95it/s, loss=3.5225, cls_loss=1.0080, bbox_loss=0.1083, giou_loss=0.6314, lr=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 3.5225, Val Loss: 2.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.9334, cls_loss=1.0688, bbox_loss=0.0863, giou_loss=0.5687, lr=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 2.9334, Val Loss: 2.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.8237, cls_loss=1.0130, bbox_loss=0.1005, giou_loss=0.5669, lr=0.000099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 2.8237, Val Loss: 2.7371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=2.7428, cls_loss=1.1858, bbox_loss=0.0904, giou_loss=0.6661, lr=0.000099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 2.7428, Val Loss: 2.6899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.7078, cls_loss=0.8237, bbox_loss=0.1136, giou_loss=0.5842, lr=0.000098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 2.7078, Val Loss: 2.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=2.5883, cls_loss=1.0018, bbox_loss=0.0727, giou_loss=0.5251, lr=0.000096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 2.5883, Val Loss: 2.4594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.5054, cls_loss=0.9383, bbox_loss=0.0672, giou_loss=0.5017, lr=0.000095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 2.5054, Val Loss: 2.4480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.4616, cls_loss=0.9203, bbox_loss=0.1093, giou_loss=0.5601, lr=0.000093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 2.4616, Val Loss: 2.4324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.3938, cls_loss=1.0087, bbox_loss=0.0940, giou_loss=0.6106, lr=0.000091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 2.3938, Val Loss: 2.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.3553, cls_loss=0.7825, bbox_loss=0.1245, giou_loss=0.5884, lr=0.000088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 2.3553, Val Loss: 2.3606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=2.3284, cls_loss=1.1143, bbox_loss=0.0974, giou_loss=0.6147, lr=0.000086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train Loss: 2.3284, Val Loss: 2.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=2.2423, cls_loss=1.0549, bbox_loss=0.0891, giou_loss=0.5310, lr=0.000083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train Loss: 2.2423, Val Loss: 2.1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=2.2201, cls_loss=0.8458, bbox_loss=0.0674, giou_loss=0.4869, lr=0.000080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train Loss: 2.2201, Val Loss: 2.1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.1789, cls_loss=0.8712, bbox_loss=0.0776, giou_loss=0.4654, lr=0.000076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train Loss: 2.1789, Val Loss: 2.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.1357, cls_loss=0.5494, bbox_loss=0.0519, giou_loss=0.4299, lr=0.000073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train Loss: 2.1357, Val Loss: 2.0630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.0944, cls_loss=0.6731, bbox_loss=0.0695, giou_loss=0.5064, lr=0.000069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train Loss: 2.0944, Val Loss: 2.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=2.0445, cls_loss=0.5602, bbox_loss=0.0731, giou_loss=0.3998, lr=0.000066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train Loss: 2.0445, Val Loss: 2.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=2.0022, cls_loss=0.9120, bbox_loss=0.0897, giou_loss=0.6155, lr=0.000062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train Loss: 2.0022, Val Loss: 1.9614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.9769, cls_loss=0.6082, bbox_loss=0.0537, giou_loss=0.3786, lr=0.000058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train Loss: 1.9769, Val Loss: 1.9445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.9507, cls_loss=0.7212, bbox_loss=0.0651, giou_loss=0.4104, lr=0.000054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Loss: 1.9507, Val Loss: 1.9164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.9277, cls_loss=0.6981, bbox_loss=0.0521, giou_loss=0.4028, lr=0.000050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - Train Loss: 1.9277, Val Loss: 1.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.9028, cls_loss=0.6601, bbox_loss=0.0554, giou_loss=0.4493, lr=0.000047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - Train Loss: 1.9028, Val Loss: 1.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.8662, cls_loss=0.5877, bbox_loss=0.0562, giou_loss=0.4091, lr=0.000043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - Train Loss: 1.8662, Val Loss: 1.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.8506, cls_loss=0.9694, bbox_loss=0.0679, giou_loss=0.4509, lr=0.000039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - Train Loss: 1.8506, Val Loss: 1.8236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.8226, cls_loss=0.7908, bbox_loss=0.0631, giou_loss=0.4274, lr=0.000035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - Train Loss: 1.8226, Val Loss: 1.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.8037, cls_loss=0.5671, bbox_loss=0.0439, giou_loss=0.3744, lr=0.000032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - Train Loss: 1.8037, Val Loss: 1.7684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.7924, cls_loss=0.6931, bbox_loss=0.0550, giou_loss=0.3912, lr=0.000028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - Train Loss: 1.7924, Val Loss: 1.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.7794, cls_loss=0.6833, bbox_loss=0.0501, giou_loss=0.3708, lr=0.000025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - Train Loss: 1.7794, Val Loss: 1.7648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.7497, cls_loss=0.5445, bbox_loss=0.0429, giou_loss=0.4222, lr=0.000021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - Train Loss: 1.7497, Val Loss: 1.7356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.7304, cls_loss=0.6088, bbox_loss=0.0519, giou_loss=0.4305, lr=0.000018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Train Loss: 1.7304, Val Loss: 1.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.7229, cls_loss=0.7557, bbox_loss=0.0748, giou_loss=0.4196, lr=0.000015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - Train Loss: 1.7229, Val Loss: 1.6983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.7040, cls_loss=0.7450, bbox_loss=0.0640, giou_loss=0.4477, lr=0.000013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - Train Loss: 1.7040, Val Loss: 1.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.6925, cls_loss=0.5593, bbox_loss=0.0631, giou_loss=0.4268, lr=0.000010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - Train Loss: 1.6925, Val Loss: 1.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.6827, cls_loss=0.6573, bbox_loss=0.0555, giou_loss=0.4781, lr=0.000008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - Train Loss: 1.6827, Val Loss: 1.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.6728, cls_loss=0.6421, bbox_loss=0.0448, giou_loss=0.3471, lr=0.000006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - Train Loss: 1.6728, Val Loss: 1.6756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.6674, cls_loss=0.6378, bbox_loss=0.0583, giou_loss=0.3901, lr=0.000005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - Train Loss: 1.6674, Val Loss: 1.6636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|| 748/748 [06:22<00:00,  1.96it/s, loss=1.6580, cls_loss=0.7342, bbox_loss=0.0546, giou_loss=0.4114, lr=0.000003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - Train Loss: 1.6580, Val Loss: 1.6624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.6558, cls_loss=0.5418, bbox_loss=0.0384, giou_loss=0.3455, lr=0.000002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - Train Loss: 1.6558, Val Loss: 1.6516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.6522, cls_loss=0.5672, bbox_loss=0.0554, giou_loss=0.4449, lr=0.000002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - Train Loss: 1.6522, Val Loss: 1.6549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|| 748/748 [06:21<00:00,  1.96it/s, loss=1.6530, cls_loss=0.5487, bbox_loss=0.0566, giou_loss=0.3498, lr=0.000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - Train Loss: 1.6530, Val Loss: 1.6628\n"
     ]
    }
   ],
   "source": [
    "# Modify training parameters\n",
    "trained_model = train_ijepa_detector(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=40,  \n",
    "    initial_lr=1e-4,  \n",
    "    device=device,\n",
    "    unfreeze_backbone_epoch=5,\n",
    "    max_objects=MAX_OBJECTS,\n",
    "    patience=15,  \n",
    "    gradient_clip_val=0.1  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451e2e73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:57:32.752486Z",
     "iopub.status.busy": "2024-12-17T14:57:32.752073Z",
     "iopub.status.idle": "2024-12-17T14:59:16.608113Z",
     "shell.execute_reply": "2024-12-17T14:59:16.606711Z"
    },
    "papermill": {
     "duration": 106.811571,
     "end_time": "2024-12-17T14:59:16.610725",
     "exception": false,
     "start_time": "2024-12-17T14:57:29.799154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 188/188 [01:43<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detection Results:\n",
      "Overall Mean IOU: 0.7104\n",
      "Total detections: 6615\n",
      "\n",
      "Per-class Results:\n",
      "Car:\n",
      "  Accuracy: 0.8694\n",
      "  IoU: 0.7201\n",
      "  Total detections: 5131\n",
      "Van:\n",
      "  Accuracy: 0.4491\n",
      "  IoU: 0.6900\n",
      "  Total detections: 452\n",
      "Truck:\n",
      "  Accuracy: 0.6579\n",
      "  IoU: 0.7199\n",
      "  Total detections: 190\n",
      "Pedestrian:\n",
      "  Accuracy: 0.6921\n",
      "  IoU: 0.6581\n",
      "  Total detections: 484\n",
      "Person_sitting:\n",
      "  Accuracy: 0.5652\n",
      "  IoU: 0.6554\n",
      "  Total detections: 23\n",
      "Cyclist:\n",
      "  Accuracy: 0.4726\n",
      "  IoU: 0.6451\n",
      "  Total detections: 146\n",
      "Tram:\n",
      "  Accuracy: 0.8052\n",
      "  IoU: 0.7011\n",
      "  Total detections: 77\n",
      "Misc:\n",
      "  Accuracy: 0.4554\n",
      "  IoU: 0.6592\n",
      "  Total detections: 112\n",
      "\n",
      "Overall Results:\n",
      "Mean IoU: 0.7104\n",
      "Total detections: 6615\n",
      "\n",
      "Results by Class:\n",
      "Car:\n",
      "  Accuracy: 0.8694\n",
      "  IoU: 0.7201\n",
      "Van:\n",
      "  Accuracy: 0.4491\n",
      "  IoU: 0.6900\n",
      "Truck:\n",
      "  Accuracy: 0.6579\n",
      "  IoU: 0.7199\n",
      "Pedestrian:\n",
      "  Accuracy: 0.6921\n",
      "  IoU: 0.6581\n",
      "Person_sitting:\n",
      "  Accuracy: 0.5652\n",
      "  IoU: 0.6554\n",
      "Cyclist:\n",
      "  Accuracy: 0.4726\n",
      "  IoU: 0.6451\n",
      "Tram:\n",
      "  Accuracy: 0.8052\n",
      "  IoU: 0.7011\n",
      "Misc:\n",
      "  Accuracy: 0.4554\n",
      "  IoU: 0.6592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IOU between box1 and box2\n",
    "    Boxes are in format: (center_x, center_y, width, height)\n",
    "    \"\"\"\n",
    "    # Convert to x1, y1, x2, y2 format\n",
    "    box1_x1 = box1[0] - box1[2]/2\n",
    "    box1_y1 = box1[1] - box1[3]/2\n",
    "    box1_x2 = box1[0] + box1[2]/2\n",
    "    box1_y2 = box1[1] + box1[3]/2\n",
    "    \n",
    "    box2_x1 = box2[0] - box2[2]/2\n",
    "    box2_y1 = box2[1] - box2[3]/2\n",
    "    box2_x2 = box2[0] + box2[2]/2\n",
    "    box2_y2 = box2[1] + box2[3]/2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x1 = max(box1_x1, box2_x1)\n",
    "    y1 = max(box1_y1, box2_y1)\n",
    "    x2 = min(box1_x2, box2_x2)\n",
    "    y2 = min(box1_y2, box2_y2)\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # Calculate union\n",
    "    box1_area = box1[2] * box1[3]\n",
    "    box2_area = box2[2] * box2[3]\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / (union + 1e-6)\n",
    "    \n",
    "def evaluate_detection(model, test_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    total_ious = []\n",
    "    class_ious = {i: [] for i in range(8)}\n",
    "    class_metrics = {i: {'correct': 0, 'total': 0, 'ious': []} for i in range(8)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, target_boxes, target_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            cls_preds, bbox_preds = model(images)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                scores = F.softmax(cls_preds[i], dim=-1)\n",
    "                confidences, pred_labels = scores.max(dim=-1)\n",
    "                pred_boxes = bbox_preds[i]\n",
    "                \n",
    "                mask = confidences > 0.3\n",
    "                filtered_boxes = pred_boxes[mask]\n",
    "                filtered_labels = pred_labels[mask]\n",
    "                \n",
    "                if len(target_boxes[i]) > 0 and len(filtered_boxes) > 0:\n",
    "                    for gt_box, gt_label in zip(target_boxes[i], target_labels[i]):\n",
    "                        max_iou = 0\n",
    "                        best_pred_label = None\n",
    "                        \n",
    "                        for pred_box, pred_label in zip(filtered_boxes, filtered_labels):\n",
    "                            iou = calculate_iou(gt_box.cpu().numpy(), pred_box.cpu().numpy())\n",
    "                            if iou > max_iou:\n",
    "                                max_iou = iou\n",
    "                                best_pred_label = pred_label.item()\n",
    "                        \n",
    "                        gt_label = gt_label.item()\n",
    "                        if max_iou >= iou_threshold:\n",
    "                            total_ious.append(max_iou)\n",
    "                            class_ious[gt_label].append(max_iou)\n",
    "                            class_metrics[gt_label]['total'] += 1\n",
    "                            if best_pred_label == gt_label:\n",
    "                                class_metrics[gt_label]['correct'] += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    mean_iou = np.mean(total_ious) if total_ious else 0\n",
    "    class_mean_ious = {k: np.mean(v) if v else 0 for k, v in class_ious.items()}\n",
    "    class_accuracies = {k: v['correct']/v['total'] if v['total'] > 0 else 0 \n",
    "                       for k, v in class_metrics.items()}\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nDetection Results:\")\n",
    "    print(f\"Overall Mean IOU: {mean_iou:.4f}\")\n",
    "    print(f\"Total detections: {len(total_ious)}\")\n",
    "    print(\"\\nPer-class Results:\")\n",
    "    for class_id in class_metrics:\n",
    "        class_name = IDX_TO_CLASS[class_id]\n",
    "        accuracy = class_accuracies[class_id]\n",
    "        iou = class_mean_ious[class_id]\n",
    "        total = class_metrics[class_id]['total']\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  IoU: {iou:.4f}\")\n",
    "        print(f\"  Total detections: {total}\")\n",
    "\n",
    "    return {\n",
    "        'mean_iou': mean_iou,\n",
    "        'class_ious': class_mean_ious,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'total_detections': len(total_ious)\n",
    "    }\n",
    "IDX_TO_CLASS = {v: k for k, v in KITTIDataset.CLASSES.items()}\n",
    "\n",
    "# Use the function\n",
    "results = evaluate_detection(model, val_loader, device)\n",
    "\n",
    "# Now you can evaluate\n",
    "print(f\"\\nOverall Results:\")\n",
    "print(f\"Mean IoU: {results['mean_iou']:.4f}\")\n",
    "print(f\"Total detections: {results['total_detections']}\")\n",
    "\n",
    "print(\"\\nResults by Class:\")\n",
    "for class_id, acc in results['class_accuracies'].items():\n",
    "    class_name = IDX_TO_CLASS[class_id]\n",
    "    iou = results['class_ious'][class_id]\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  IoU: {iou:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 976194,
     "sourceId": 1650695,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19322.347851,
   "end_time": "2024-12-17T14:59:21.896079",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-17T09:37:19.548228",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
